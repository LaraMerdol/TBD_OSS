{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Reddit**"
      ],
      "metadata": {
        "id": "bdsv_8vVOmUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "import asyncio\n",
        "import asyncpraw\n",
        "import asyncprawcore\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# --- Patterns ---\n",
        "TBD_KEYWORDS = [\n",
        "    \"trunk based development\", \"trunk-based\", \"mainline development\",\n",
        "    \"GitFlow vs trunk\", \"trunk strategy\", \"short-lived branches\"\n",
        "]\n",
        "\n",
        "TBD_PATTERNS = [\n",
        "    r\"\\btrunk[-\\s]?based\\sdevelopment\\b\",\n",
        "    r\"\\btrunk[-\\s]?based\\b\",\n",
        "    r\"\\bmainline\\sdevelopment\\b\",\n",
        "    r\"\\bshort[-\\s]?lived\\sbranches?\\b\",\n",
        "    r\"\\bGitFlow\\s+vs\\s+trunk\\b\",\n",
        "    r\"\\btrunk\\sstrategy\\b\",\n",
        "    r\"\\btrunk vs\\b\"\n",
        "]\n",
        "\n",
        "OSS_PATTERNS = [\n",
        "    r\"\\bopen source\\b\", r\"\\bOSS\\b\", r\"\\bFOSS\\b\", r\"\\bMIT license\\b\", r\"\\bpublic repo\\b\"\n",
        "]\n",
        "\n",
        "SUBREDDITS = [\n",
        "    'devops', 'git', 'softwaredevelopment', 'programming',\n",
        "    'cscareerquestions', 'learnprogramming', 'webdev', 'softwareengineering'\n",
        "]\n",
        "\n",
        "# --- Helpers ---\n",
        "def is_tbd_related(text):\n",
        "    return any(re.search(p, text, re.IGNORECASE) for p in TBD_PATTERNS)\n",
        "\n",
        "def is_oss(text):\n",
        "    return any(re.search(p, text, re.IGNORECASE) for p in OSS_PATTERNS)\n",
        "\n",
        "def clean_html(text):\n",
        "    return re.sub('<[^<]+?>', '', text).strip()\n",
        "\n",
        "def save_rows(rows, prefix):\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv(f\"{prefix}.csv\", index=False)\n",
        "    df.to_json(f\"{prefix}.json\", orient=\"records\", indent=2)\n",
        "    print(f\"üíæ Saved {len(rows)} rows to {prefix}.*\")\n",
        "\n",
        "# --- Main Function ---\n",
        "async def fetch_reddit_threads_raw():\n",
        "    reddit = asyncpraw.Reddit(\n",
        "        client_id='5sbtbAgx0CFLQqjwWrK0sg',\n",
        "        client_secret='9noISNLsQ3TChIEu9P5OkdWxJBMzUg',\n",
        "        user_agent='tbd-research-script by /u/Relevant-Egg9675'\n",
        "    )\n",
        "\n",
        "    seen_ids = set()\n",
        "    all_rows = []\n",
        "    try:\n",
        "        for keyword in TBD_KEYWORDS:\n",
        "            print(f\"üîç Searching Reddit for keyword: '{keyword}'\")\n",
        "            for subreddit_name in SUBREDDITS:\n",
        "                subreddit = await reddit.subreddit(subreddit_name)\n",
        "                async for submission in subreddit.search(keyword, sort='new', time_filter='all', limit=None):\n",
        "                    if submission.id in seen_ids:\n",
        "                        continue\n",
        "                    seen_ids.add(submission.id)\n",
        "\n",
        "                    await submission.load()\n",
        "\n",
        "                    # ‚úÖ Only keep submission if title or selftext match TBD\n",
        "                    if not (is_tbd_related(submission.title) or is_tbd_related(submission.selftext)):\n",
        "                        continue\n",
        "\n",
        "                    await submission.comments.replace_more(limit=0)\n",
        "\n",
        "                    discussion_items = [submission.title.strip(), submission.selftext.strip()]\n",
        "                    for comment in submission.comments.list():\n",
        "                        if isinstance(comment, asyncpraw.models.Comment):\n",
        "                            discussion_items.append(comment.body.strip())\n",
        "\n",
        "                    filtered_items = []\n",
        "                    oss_flag = False\n",
        "\n",
        "                    for item in discussion_items:\n",
        "                        if not item.strip():\n",
        "                            continue\n",
        "                        text = clean_html(item)\n",
        "                        if is_oss(text):\n",
        "                            oss_flag = True\n",
        "                        if is_tbd_related(text):\n",
        "                            filtered_items.append(text)\n",
        "\n",
        "                    if not filtered_items:\n",
        "                        continue\n",
        "\n",
        "                    row = {\n",
        "                        \"keyword\": keyword,\n",
        "                        \"subreddit\": subreddit_name,\n",
        "                        \"title\": submission.title,\n",
        "                        \"url\": submission.url,\n",
        "                        \"permalink\": f\"https://www.reddit.com{submission.permalink}\",\n",
        "                        \"OSS-related?\": oss_flag,\n",
        "                        \"discussion_items\": filtered_items\n",
        "                    }\n",
        "\n",
        "                    all_rows.append(row)\n",
        "\n",
        "                    # Optional checkpoint every 50\n",
        "                    if len(all_rows) % 50 == 0:\n",
        "                        save_rows(all_rows, \"reddit_tbd_checkpoint\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error: {e}\")\n",
        "        save_rows(all_rows, \"reddit_tbd_error_dump\")\n",
        "\n",
        "    save_rows(all_rows, \"reddit_tbd_raw_full\")\n",
        "    print(\"‚úÖ Done.\")\n",
        "\n",
        "\n",
        "# --- Run in Notebook ---\n",
        "await fetch_reddit_threads_raw()\n",
        "# For script version: asyncio.run(fetch_reddit_threads_raw())\n"
      ],
      "metadata": {
        "id": "tdWKwYl-O6D9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a4aa542-9ee6-4e8b-b6f3-01c5f3ff545e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Searching Reddit for keyword: 'trunk based development'\n",
            "üíæ Saved 50 rows to reddit_tbd_checkpoint.*\n",
            "‚ö†Ô∏è Error: received 429 HTTP response\n",
            "üíæ Saved 78 rows to reddit_tbd_error_dump.*\n",
            "üíæ Saved 78 rows to reddit_tbd_raw_full.*\n",
            "‚úÖ Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stack Overflow**"
      ],
      "metadata": {
        "id": "h5nXiReLOmwk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRUeCzvXOklF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adef0854-0c0b-424e-e91b-56adc771d516"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Searching Reddit for: 'trunk based development'\n",
            "‚ö†Ô∏è Request failed with status 403\n",
            "üîç Searching Reddit for: 'trunk-based'\n",
            "‚ö†Ô∏è Request failed with status 403\n",
            "üîç Searching Reddit for: 'mainline development'\n",
            "‚ö†Ô∏è Request failed with status 403\n",
            "üîç Searching Reddit for: 'GitFlow vs trunk'\n",
            "‚ö†Ô∏è Request failed with status 403\n",
            "üîç Searching Reddit for: 'trunk strategy'\n",
            "‚ö†Ô∏è Request failed with status 403\n",
            "üîç Searching Reddit for: 'short-lived branches'\n",
            "‚ö†Ô∏è Request failed with status 403\n",
            "‚úÖ Done! Saved 0 threads.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import re\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# --- Patterns ---\n",
        "TBD_KEYWORDS = [\n",
        "    \"trunk based development\", \"trunk-based\", \"mainline development\",\n",
        "    \"GitFlow vs trunk\", \"trunk strategy\", \"short-lived branches\"\n",
        "]\n",
        "\n",
        "TBD_PATTERNS = [\n",
        "    r\"\\btrunk[-\\s]?based\\sdevelopment\\b\",\n",
        "    r\"\\btrunk[-\\s]?based\\b\",\n",
        "    r\"\\bmainline\\sdevelopment\\b\",\n",
        "    r\"\\bshort[-\\s]?lived\\sbranches?\\b\",\n",
        "    r\"\\bGitFlow\\s+vs\\s+trunk\\b\",\n",
        "    r\"\\btrunk\\sstrategy\\b\",\n",
        "    r\"\\btrunk vs\\b\"\n",
        "]\n",
        "\n",
        "OSS_PATTERNS = [\n",
        "    r\"\\bopen source\\b\", r\"\\bOSS\\b\", r\"\\bFOSS\\b\", r\"\\bMIT license\\b\", r\"\\bpublic repo\\b\"\n",
        "]\n",
        "\n",
        "STACK_API = \"https://api.stackexchange.com/2.3\"\n",
        "\n",
        "def is_tbd_related(text):\n",
        "    return any(re.search(p, text, re.IGNORECASE) for p in TBD_PATTERNS)\n",
        "\n",
        "def is_oss(text):\n",
        "    return any(re.search(p, text, re.IGNORECASE) for p in OSS_PATTERNS)\n",
        "\n",
        "def clean_html(raw_html):\n",
        "    return re.sub('<[^<]+?>', '', raw_html).strip()\n",
        "\n",
        "def fetch_stackoverflow_discussions():\n",
        "    all_rows = []\n",
        "    for keyword in TBD_KEYWORDS:\n",
        "        print(f\"üîç Querying: {keyword}\")\n",
        "        page = 1\n",
        "        while True:\n",
        "            url = f\"{STACK_API}/search/advanced\"\n",
        "            params = {\n",
        "                \"order\": \"desc\",\n",
        "                \"sort\": \"relevance\",\n",
        "                \"q\": keyword,\n",
        "                \"site\": \"stackoverflow\",\n",
        "                \"filter\": \"withbody\",\n",
        "                \"pagesize\": 100,\n",
        "                \"page\": page\n",
        "            }\n",
        "\n",
        "            res = requests.get(url, params=params)\n",
        "            if res.status_code != 200:\n",
        "                print(\"‚ö†Ô∏è Failed request\")\n",
        "                break\n",
        "\n",
        "            data = res.json()\n",
        "            questions = data.get(\"items\", [])\n",
        "            print(f\"üîπ Page {page}: {len(questions)} results\")\n",
        "\n",
        "            for q in questions:\n",
        "                question_id = q.get(\"question_id\")\n",
        "                title = q.get(\"title\", \"\")\n",
        "                link = q.get(\"link\")\n",
        "                body = q.get(\"body\", \"\")\n",
        "\n",
        "                discussion_items = [title.strip(), body.strip()]\n",
        "\n",
        "                # Fetch answers\n",
        "                ans_url = f\"{STACK_API}/questions/{question_id}/answers\"\n",
        "                ans_params = {\n",
        "                    \"order\": \"desc\",\n",
        "                    \"sort\": \"votes\",\n",
        "                    \"site\": \"stackoverflow\",\n",
        "                    \"filter\": \"withbody\",\n",
        "                    \"pagesize\": 100\n",
        "                }\n",
        "                ans_res = requests.get(ans_url, params=ans_params)\n",
        "                answers = ans_res.json().get(\"items\", []) if ans_res.status_code == 200 else []\n",
        "\n",
        "                for a in answers:\n",
        "                    answer_body = a.get(\"body\", \"\")\n",
        "                    if answer_body:\n",
        "                        discussion_items.append(answer_body.strip())\n",
        "\n",
        "                filtered_items = []\n",
        "                oss_flag = False\n",
        "                for item in discussion_items:\n",
        "                    clean = clean_html(item)\n",
        "                    if not clean:\n",
        "                        continue\n",
        "                    if is_oss(clean):\n",
        "                        oss_flag = True\n",
        "                    if is_tbd_related(clean):\n",
        "                        filtered_items.append(clean)\n",
        "                    else:\n",
        "                        filtered_items.append(clean + \" (new)\")\n",
        "\n",
        "                if not filtered_items:\n",
        "                    continue\n",
        "\n",
        "                row = {\n",
        "                    \"title\": title,\n",
        "                    \"question_id\": question_id,\n",
        "                    \"url\": link,\n",
        "                    \"OSS-related?\": oss_flag,\n",
        "                    \"discussion_items\": filtered_items\n",
        "                }\n",
        "\n",
        "                all_rows.append(row)\n",
        "                time.sleep(0.2)\n",
        "\n",
        "            if not data.get(\"has_more\"):\n",
        "                break\n",
        "            page += 1\n",
        "            time.sleep(0.5)  # Be polite with the API\n",
        "\n",
        "    df = pd.DataFrame(all_rows)\n",
        "    df.to_csv(\"stackoverflow_tbd_raw_full.csv\", index=False)\n",
        "    df.to_json(\"stackoverflow_tbd_raw_full.json\", orient=\"records\", indent=2)\n",
        "    print(\"‚úÖ Saved all Stack Overflow results across pages.\")\n",
        "\n",
        "# --- Run ---\n",
        "fetch_stackoverflow_discussions()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HackerNews**"
      ],
      "metadata": {
        "id": "AjfCcHAPOm8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import re\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# --- Keywords and Patterns ---\n",
        "TBD_KEYWORDS = [\n",
        "    \"trunk based development\", \"trunk-based development\",\n",
        "    \"mainline development\", \"GitFlow vs trunk\",\n",
        "    \"short-lived branches\", \"trunk strategy\"\n",
        "]\n",
        "\n",
        "TBD_PATTERNS = [\n",
        "    r\"\\btrunk[-\\s]?based\\sdevelopment\\b\",\n",
        "    r\"\\btrunk[-\\s]?based\\b\",\n",
        "    r\"\\bmainline\\sdevelopment\\b\",\n",
        "    r\"\\bshort[-\\s]?lived\\sbranches?\\b\",\n",
        "    r\"\\bGitFlow\\s+vs\\s+trunk\\b\",\n",
        "    r\"\\btrunk\\sstrategy\\b\",\n",
        "    r\"\\btrunk vs\\b\"\n",
        "]\n",
        "\n",
        "OSS_PATTERNS = [r\"\\bopen source\\b\", r\"\\bOSS\\b\", r\"\\bFOSS\\b\", r\"\\bMIT license\\b\", r\"\\bpublic repo\\b\"]\n",
        "\n",
        "def is_tbd_related(text):\n",
        "    return any(re.search(p, text, re.IGNORECASE) for p in TBD_PATTERNS)\n",
        "\n",
        "def is_oss(text):\n",
        "    return any(re.search(p, text, re.IGNORECASE) for p in OSS_PATTERNS)\n",
        "\n",
        "def clean_html(raw_html):\n",
        "    return re.sub('<[^<]+?>', '', raw_html).strip()\n",
        "\n",
        "def fetch_item(item_id):\n",
        "    url = f\"https://hacker-news.firebaseio.com/v0/item/{item_id}.json\"\n",
        "    response = requests.get(url)\n",
        "    return response.json()\n",
        "\n",
        "def fetch_hn_raw_discussions():\n",
        "    all_rows = []\n",
        "\n",
        "    for keyword in TBD_KEYWORDS:\n",
        "        print(f\"üîç Searching HN for: {keyword}\")\n",
        "        page = 0\n",
        "        while True:\n",
        "            search_url = (\n",
        "                f\"https://hn.algolia.com/api/v1/search?\"\n",
        "                f\"query={keyword}&tags=story&page={page}&hitsPerPage=100\"\n",
        "            )\n",
        "            response = requests.get(search_url)\n",
        "            data = response.json()\n",
        "            hits = data.get(\"hits\", [])\n",
        "            if not hits:\n",
        "                break  # No more results\n",
        "            print(f\"üîπ Page {page} -> {len(hits)} hits\")\n",
        "\n",
        "            for hit in hits:\n",
        "                item_id = hit.get(\"objectID\")\n",
        "                title = hit.get(\"title\", \"\")\n",
        "                story_text = hit.get(\"story_text\", \"\") or hit.get(\"comment_text\", \"\") or \"\"\n",
        "                url = hit.get(\"url\", f\"https://news.ycombinator.com/item?id={item_id}\")\n",
        "\n",
        "                # Fetch top-level comments\n",
        "                discussion_items = []\n",
        "                full_item = fetch_item(item_id)\n",
        "                if full_item:\n",
        "                    comment_ids = full_item.get(\"kids\", [])[:20]  # Limit to 20 comments\n",
        "                    for cid in comment_ids:\n",
        "                        comment = fetch_item(cid)\n",
        "                        if comment and \"text\" in comment:\n",
        "                            discussion_items.append(comment[\"text\"])\n",
        "\n",
        "                raw_items = [title, story_text] + discussion_items\n",
        "                filtered_items = []\n",
        "                oss_flag = False\n",
        "\n",
        "                for item in raw_items:\n",
        "                    clean = clean_html(item)\n",
        "                    if not clean:\n",
        "                        continue\n",
        "                    if is_oss(clean):\n",
        "                        oss_flag = True\n",
        "                    if is_tbd_related(clean):\n",
        "                        filtered_items.append(clean)\n",
        "                    else:\n",
        "                        filtered_items.append(clean + \" (new)\")\n",
        "\n",
        "                if not filtered_items:\n",
        "                    continue\n",
        "\n",
        "                row = {\n",
        "                    \"title\": title,\n",
        "                    \"id\": item_id,\n",
        "                    \"url\": url,\n",
        "                    \"OSS-related?\": oss_flag,\n",
        "                    \"discussion_items\": filtered_items\n",
        "                }\n",
        "\n",
        "                all_rows.append(row)\n",
        "                time.sleep(0.2)\n",
        "\n",
        "            page += 1\n",
        "            time.sleep(0.5)\n",
        "\n",
        "    # Save results\n",
        "    df = pd.DataFrame(all_rows)\n",
        "    df.to_csv(\"hackernews_tbd_raw_full.csv\", index=False)\n",
        "    df.to_json(\"hackernews_tbd_raw_full.json\", orient=\"records\", indent=2)\n",
        "    print(\"‚úÖ Completed. Saved all paginated Hacker News results.\")\n",
        "\n",
        "# --- Run ---\n",
        "fetch_hn_raw_discussions()\n"
      ],
      "metadata": {
        "id": "vNkXQ3xlO86R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0ba3eca-7b6c-4a10-c7cb-d3e68823d011"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Searching HN for: trunk based development\n",
            "üîπ Page 0 -> 84 hits\n",
            "üîç Searching HN for: trunk-based development\n",
            "üîπ Page 0 -> 83 hits\n",
            "üîç Searching HN for: mainline development\n",
            "üîπ Page 0 -> 9 hits\n",
            "üîç Searching HN for: GitFlow vs trunk\n",
            "üîπ Page 0 -> 1 hits\n",
            "üîç Searching HN for: short-lived branches\n",
            "üîπ Page 0 -> 3 hits\n",
            "üîç Searching HN for: trunk strategy\n",
            "üîπ Page 0 -> 7 hits\n",
            "‚úÖ Completed. Saved all paginated Hacker News results.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gerrit OpenStack** (Reurn O)"
      ],
      "metadata": {
        "id": "aEZhCE_DPCXY"
      }
    }
  ]
}